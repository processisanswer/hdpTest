列举hadoop常用的一些InputFormat

InputFormat是用来对我们的输入数据进行格式化的，
TextInputFormat是默认的
InputFormat有哪些类型?
DBInputFormat DelegatingInputFormat
FileInputFormat，常用的就是DBInputFormat、FileInputFormat

DBInputFormat：接我们的关系型数据库，比如mysql和oracle
FileInputFormat是和文件相关的，又有很多子类
    TextInputFormat，最常见，处理普通的文件，一行是一条记录
    CombinerFileInputFormat，多个输入文件，格式可不一样，
        例如里边可有普通文件、SequenceFile文件
    KeyValueTextInputFormat：和TextInputFormat不同，TextInputFormat
    中的key表示的是行的偏移量，单位是字节，我们的value是我们行文本
    的内容，KeyValueTextInputFormat表示key和value是通过第一个
    制表符划分的
    NLineInputFormat：指的是把多少行作为一条记录，可指定2行是一条
    记录还是3行一条记录。
    SequenceFileInputFormat：数据存放格式叫作SequenceFile，不是
    我们的普通的文本文件，是hadoop中自己定义的一种比较高效的数据
    存储模式、节省空间、处理效率也高。
-----------------
    TextInputFormat和KeyValueTextInputFormat的不同之处
    key、value值不一样。
    NLineInputFormat：允许处理多行。粒度更粗、处理速度相对快
    KeyValueTextInputFormat：和数据内容有关
    CombinerFileInputFormat：多种格式在一起
    SequenceFileInputFormat:数据格式仅限于SequenceFile
OutputFormat子类：
    DBOutputFormat:指的是要写到数据库中的
    FileOutputFormat：使用最多
    TextOutputFormat、SequenceFileOutputFormat:也是数据格式不一样
    NullOutputFormat:指的是什么也不输出，有可能只是做处理。

-----hadoop中InputSplit是什么----
InputSplit 是在MapReduce中，map阶段，每个map任务单独处理的
一个数据单位，默认情况下，与block size 一样大。
InputSplit是我们MapReduce范畴，在hdfs中叫block，可以决定单个
map任务处理的数据量大小。在wordCount例子中使用的是InputSplit的
一个子类，叫FileInputSplit。
--什么是JobTracker，JobTracker有哪些特殊函数?--
    jobTracker是一个RPC服务端，监控和管理TaskTracker。
    对外开发的接口有
    JobSubmissionProtocol
        是客户端用来把作业提交给JobTracker的
        里边有submit()方法，有getJobId()方法
    TaskTrackerProtocol
        是TaskTracker与JobTracker通信用的，有个很重要的方法
        sendHeartBeat()发送心跳方法。
--什么是TaskTracker？--
    TaskTracker是我们MapReduce的客户端，主从式结构从的那部分。
    在MapReduce中可以有很多节点，TaskTracker是用来接收JobTracker
    发送的指令，并且执行任务的。

--hadoop中的job和task是什么关系？--
    我们跑一次程序就是一个Job。job运行的时候，至少划分为
    MapTask和 ReduceTask，Task是Job运行作业重要的组成部分，
    只有不同的task连续运行完成之后，我们的job才会运行完。
--假设hadoop一个job产生了100个task，并且其中的一个task失败了
，hadoop会如何处理？--
    hadoop有容错机制。在mapred-default.xml文件中会有mapreduce
    的一个重试机制。
    mapred.map.max.attemps设置的就是mapred中每一个map任务最大
    的尝试次数。mapred.reduce.max.attemps设置的是reduce的重试次数。
    一次失败后，JobTracker还是会发送命令让它再次执行。一般会重试4次，如果4次都失败了，则放弃，整个job任务失败。
    放弃之后不会去别的机器上启动task。

--hadoop的推测执行是如何实现的？--
    多个map和多个reduce运行的时候，如果有一个map任务运行的特别慢，
    他拖累了整个运行的速度的话，
    那么我们hadoop就会检测这个程序在相同时间内处理的数据量太少了，可能
    以后会处理不完，hadoop就会在其他节点起一个程序，来处理相同的数据，看
    谁处理的快，谁快就用谁的结果，跑的慢的那个就被杀死了，推测执行有的时候
    是好的，有的时候是差的。
    -----
    在hive中通常会去关闭推测式执行，因为它会占用很多的资源。
    ----
    mapreduce的推测式执行配置:
    map配置mapred.map.task.speculative.execution,true则会有
        很多map任务并行执行
    reduce配置mapred.reduce.task.speculative.execution

--linux命令行查看hadoop运行任务 并kill掉任务？--

    hadoop job回车就会打印出很多子命令

    -submit
    -status
    -kill
    -list
    -fail
    -history

    hadoop job -list:查看所有任务列表
    hadoop job -kill job-id： 杀死一个job
 --InputSplit、RecordRead是干嘛的？--
    MapReduce读取我们HDFS内容，把每一行转换成一个个的键值对，
    供map函数使用。
    数据文件就是通过RecordRead解析器来转化成<k1,v1>键值对的。
    RecordRead:把我们输入数据文本文件中每一行转换为一个个的键值对。
    InputSplit就是我们mapreduce对hdfs数据的划分，一个InputSplit
    默认情况下对应一个block，每一个InputSplit对应一个map，InputSplit
    这个概念是属于MapReduce这个领域的。

    InputSplit和RecordRead是我们mapReduce领域数据处理阶段
    非常重要的概念。

    InputSplit的作用是对原始输入数据的一个封装，封装的是外部数据源
    的数据，RecordRead就是做转换的，把我们的InputSplit转换为一个个
    的键值对就达到目的了。如果数据源是hdfs，默认是block。
    数据源可以是关系型DB，也可以是一些数据流。
    InputSplit和block除了大小一样之外，没有联系。
    map任务的数量是有我们InputSplit的数据决定的。

--InputSplit是如何从hdfs产生的？如何将hdfs中文件转换为一个个
的inputSplit列表，LineRecordReader里边到底是如何把InputSplit中的
数据转换为键值对的？--
    FileInputFormat中的getSplits()，将我们的输入转换成InputSplit，
    产生了一些文件里列表，并且把他们做成FileSplit。
    FileSplit是InputSplit的子类
    可通过配置项配置InputSplit的最小值和最大值
--为什么hadoop不适合处理小文件--
    每个输入文件必须有一个切片，每一个小文件都是一个切片，意味着
    就会有很多的map
    block大小默认是64M
    FileSplit中存放的是我们的文件路径、起始位置、处理的数据长度。
    FileSplit本身不存储数据，只是存储了原始数据的位置和要处理的长度。
    RecordRead真要把输入数据的每一行转化为键值对的话，还是要去
    读hdfs，InputSplit只是做了一个封装，封装了它的位置。

















